{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5VEL9jOPKIO0jR6BJuFRg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoboiwatsup/MachineLearning/blob/main/UAS/Generative_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Generative Model\n",
        "\n",
        "1. **Definisi Model Generatif**:\n",
        "   - Model generatif adalah jenis model yang belajar untuk memahami distribusi data dan dapat menghasilkan data baru yang mirip dengan data pelatihan. Ini berbeda dengan model diskriminatif yang fokus pada pemisahan kelas.\n",
        "\n",
        "2. **Perbedaan antara Model Generatif dan Diskriminatif**:\n",
        "   - Model diskriminatif belajar batasan yang memisahkan kelas, sedangkan model generatif belajar distribusi dari kelas-kelas tersebut.\n",
        "\n",
        "3. **Tipe Model Generatif**:\n",
        "   - Halaman ini membahas beberapa jenis model generatif yang umum digunakan dalam visi komputer, termasuk:\n",
        "     - **DCGAN** (Deep Convolutional Generative Adversarial Networks) untuk menghasilkan gambar dari noise.\n",
        "     - **Model Diffusi** untuk menghasilkan gambar dari teks.\n",
        "     - **StyleGAN dan CycleGAN** untuk transformasi gambar.\n",
        "\n",
        "4. **Evaluasi Model Generatif**:\n",
        "   - Menilai kualitas gambar yang dihasilkan oleh model generatif bisa sulit karena tidak ada \"ground truth\" yang jelas.\n",
        "   - **FID (FrÃ©chet Inception Distance)** adalah metrik yang umum digunakan untuk mengevaluasi model generatif. Semakin rendah nilai FID, semakin baik kualitas gambar yang dihasilkan.\n",
        "   - Metrik lain yang disebutkan termasuk SSIM (Structural Similarity Index), PSNR (Peak Signal-to-Noise Ratio), dan Inception Score.\n",
        "\n",
        "5. **Tantangan dalam Evaluasi**:\n",
        "   - Metrik seperti FID dan SSIM memiliki keterbatasan dan tidak selalu mencerminkan kualitas visual yang dirasakan oleh manusia.\n",
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "h98W7CalD3LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Autoencoders (VAEs)\n",
        "\n",
        "1. **Pengantar Autoencoders**:\n",
        "   - Autoencoders adalah jenis jaringan saraf yang digunakan untuk pembelajaran tanpa pengawasan dan pengurangan dimensi. Mereka bekerja dengan mengkodekan data input menjadi representasi berdimensi lebih rendah dan kemudian mendekodekannya kembali ke data asli, dengan tujuan meminimalkan kesalahan rekonstruksi.\n",
        "\n",
        "2. **Arsitektur Dasar**:\n",
        "   - Autoencoder terdiri dari dua komponen utama:\n",
        "     - **Encoder**: Mengubah data input menjadi representasi terkompresi atau laten.\n",
        "     - **Decoder**: Mengambil representasi terkompresi dan mencoba merekonstruksi data input asli.\n",
        "\n",
        "3. **Keterbatasan Autoencoders Tradisional**:\n",
        "   - Autoencoders tradisional tidak memiliki sifat probabilistik, yang membatasi kemampuan mereka dalam menghasilkan data baru.\n",
        "\n",
        "4. **Konsep VAEs**:\n",
        "   - VAEs memperkenalkan pendekatan probabilistik dalam pengkodean dan dekoding. Mereka memodelkan ruang laten sebagai distribusi probabilitas, biasanya diasumsikan sebagai distribusi Gaussian multivariat. Ini memungkinkan VAEs untuk menghasilkan sampel data baru dengan mengambil sampel dari distribusi yang dipelajari.\n",
        "\n",
        "5. **Ruang Laten**:\n",
        "   - Ruang laten dalam VAEs dirancang untuk menjadi representasi kontinu dan terstruktur dari data input. Ini memungkinkan interpolasi yang mudah, di mana setiap titik dalam ruang laten dapat menghasilkan output yang berbeda.\n",
        "\n",
        "6. **Matematika di Balik VAEs**:\n",
        "   - VAEs menggunakan model probabilistik dan inferensi variational. Fungsi kerugian untuk VAEs terdiri dari dua komponen:\n",
        "     - **Reconstruction Loss**: Mengukur seberapa baik model dapat merekonstruksi input.\n",
        "     - **KL Divergence**: Mengukur seberapa dekat distribusi yang dipelajari dengan distribusi prior yang dipilih (biasanya Gaussian).\n",
        "\n",
        "7. **Keseimbangan dalam Fungsi Kerugian**:\n",
        "   - VAEs berusaha untuk menemukan keseimbangan antara kehilangan rekonstruksi dan kehilangan laten. Keseimbangan ini penting untuk menghasilkan gambar yang berkualitas baik dan juga untuk menghasilkan data baru yang bervariasi.\n",
        "\n",
        "8. **Keunggulan VAEs**:\n",
        "   - VAEs tidak hanya merekonstruksi data, tetapi juga menghasilkan sampel baru dan memberikan kerangka probabilistik untuk memahami representasi laten. Ini menjadikan VAEs sangat kuat untuk tugas generatif."
      ],
      "metadata": {
        "id": "hXjWFQMsD9t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generative Adversarial Networks (GANs)\n",
        "\n",
        "1. **Pengantar GANs**:\n",
        "   - GANs adalah kelas model pembelajaran mendalam yang diperkenalkan oleh Ian Goodfellow dan rekan-rekannya pada tahun 2014. Konsep inti dari GANs adalah melatih dua jaringan secara bersamaan: **generator** dan **discriminator**.\n",
        "\n",
        "2. **Arsitektur GANs**:\n",
        "   - **Generator**: Mengambil input berupa noise acak dan menghasilkan data sintetis. Tujuannya adalah untuk menciptakan data yang cukup realistis sehingga dapat menipu discriminator.\n",
        "   - **Discriminator**: Berfungsi untuk menilai apakah sampel yang diberikan adalah data nyata (dari dataset asli) atau palsu (dihasilkan oleh generator). Tujuannya adalah untuk meningkatkan akurasi dalam membedakan antara data nyata dan yang dihasilkan.\n",
        "\n",
        "3. **Proses Pelatihan**:\n",
        "   - GANs dilatih melalui proses adversarial, di mana generator dan discriminator saling bersaing. Generator berusaha untuk menghasilkan data yang tidak dapat dibedakan dari data nyata, sementara discriminator berusaha untuk meningkatkan kemampuannya dalam membedakan antara keduanya.\n",
        "   - Fungsi objektif dalam pelatihan adalah permainan min-max, di mana generator berusaha meminimalkan probabilitas bahwa discriminator dapat mengklasifikasikan sampel yang dihasilkan sebagai palsu, sedangkan discriminator berusaha memaksimalkan probabilitas tersebut.\n",
        "\n",
        "4. **Perbandingan dengan VAEs**:\n",
        "   - GANs dan Variational Autoencoders (VAEs) adalah dua model generatif yang populer, tetapi memiliki kelebihan dan kekurangan yang berbeda:\n",
        "     - **Kualitas Gambar**: GANs cenderung menghasilkan gambar berkualitas lebih tinggi, terutama untuk data kompleks, sedangkan VAEs mungkin menghasilkan gambar yang lebih buram.\n",
        "     - **Kemudahan Pelatihan**: VAEs lebih mudah dilatih dan lebih stabil dibandingkan GANs, yang bisa sulit untuk dilatih dan rentan terhadap ketidakstabilan.\n",
        "\n",
        "5. **Aplikasi GANs**:\n",
        "   - GANs digunakan dalam berbagai aplikasi, termasuk:\n",
        "     - **Generasi Gambar**: Menghasilkan gambar realistis, seperti wajah manusia atau pemandangan.\n",
        "     - **Super-Resolusi**: Meningkatkan resolusi gambar.\n",
        "     - **Translasi Gambar-ke-Gambar**: Mengubah gambar dari satu domain ke domain lain (misalnya, dari gambar peta ke gambar nyata).\n",
        "\n",
        "6. **Tantangan dalam Pelatihan GANs**:\n",
        "   - Pelatihan GANs dapat menghadapi beberapa tantangan, seperti mode collapse, di mana generator hanya menghasilkan beberapa jenis output, dan kesulitan dalam mencapai keseimbangan antara generator dan discriminator."
      ],
      "metadata": {
        "id": "ZoLQALDdEJ6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StyleGAN Variants\n",
        "\n",
        "1. **Pengantar StyleGAN**:\n",
        "   - StyleGAN adalah modifikasi dari arsitektur GAN yang memberikan kontrol lebih besar atas proses generasi gambar. Ini memungkinkan pengguna untuk mengubah fitur gambar dengan lebih intuitif, seperti pose dan ekspresi wajah, serta detail halus seperti tekstur kulit.\n",
        "\n",
        "2. **Komponen Utama StyleGAN**:\n",
        "   - **Mapping Network**: Alih-alih mengirimkan kode laten (noise vector) langsung ke generator, StyleGAN memetakan kode tersebut melalui beberapa lapisan MLP (Multi-Layer Perceptron) untuk menghasilkan kode laten baru yang disebut `w`. Ini membantu dalam memisahkan fitur-fitur yang berbeda dalam ruang laten.\n",
        "   - **Adaptive Instance Normalization (AdaIN)**: Memungkinkan penyesuaian parameter normalisasi (rata-rata dan deviasi standar) secara dinamis berdasarkan informasi gaya dari kode laten. Ini memungkinkan generator untuk memodulasi perilakunya selama proses generasi.\n",
        "   - **Kombinasi Noise Vector**: StyleGAN menambahkan peta noise ke peta fitur di setiap blok jaringan sintesis, yang membantu dalam menghasilkan fitur stochastik yang beragam, seperti posisi rambut dan pori-pori kulit.\n",
        "\n",
        "3. **Kelemahan StyleGAN1 dan Kebutuhan untuk StyleGAN2**:\n",
        "   - Meskipun StyleGAN1 menghasilkan gambar berkualitas tinggi, ada beberapa artefak yang perlu diperbaiki, seperti artefak blob dan preferensi lokasi yang kuat. StyleGAN2 memperkenalkan perubahan untuk mengatasi masalah ini, termasuk penggunaan generator dan discriminator yang lebih baik.\n",
        "\n",
        "4. **Kelemahan StyleGAN2 dan Kebutuhan untuk StyleGAN3**:\n",
        "   - StyleGAN2 juga memiliki masalah dengan ketergantungan jaringan sintesis pada koordinat piksel absolut, yang menyebabkan efek aliasing. StyleGAN3 dirancang untuk mengatasi masalah ini, menghasilkan animasi yang lebih realistis dan halus.\n",
        "\n",
        "5. **Kasus Penggunaan StyleGAN**:\n",
        "   - **Pengeditan Gambar**: Termasuk pengisian gambar yang hilang dan transfer gaya gambar.\n",
        "   - **Aplikasi Pelindung Privasi**: Menghasilkan data sintetis untuk menggantikan informasi sensitif dan mengaburkan fitur yang dapat dikenali dalam gambar.\n",
        "   - **Eksplorasi Kreatif**: Menghasilkan desain mode dan menciptakan pengalaman virtual yang realistis."
      ],
      "metadata": {
        "id": "hm4g0m_uEdJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CycleGAN: Pengenalan\n",
        "\n",
        "1. **Definisi CycleGAN**:\n",
        "   - CycleGAN, singkatan dari Cycle-Consistent Generative Adversarial Network, adalah kerangka kerja yang dirancang untuk tugas penerjemahan gambar-ke-gambar di mana contoh berpasangan tidak tersedia. Diperkenalkan oleh Zhu et al. pada tahun 2017, CycleGAN memungkinkan model untuk belajar memetakan antara dua domain gambar tanpa memerlukan pasangan gambar yang sesuai.\n",
        "\n",
        "2. **Penerjemahan Gambar Tanpa Pasangan**:\n",
        "   - Dalam banyak skenario penerjemahan gambar, dataset sering kali tidak memiliki korespondensi langsung antara pasangan gambar. CycleGAN bekerja dengan dua set gambar yang berbeda, masing-masing mewakili gaya atau domain yang berbeda, seperti gambar kuda dan zebra, tanpa adanya pasangan langsung antara keduanya.\n",
        "\n",
        "3. **Komponen Utama CycleGAN**:\n",
        "   - **Struktur Dual GAN**: CycleGAN menggunakan dua GAN, satu untuk menerjemahkan dari satu set ke set lainnya (misalnya, dari zebra ke kuda) dan satu lagi untuk proses sebaliknya (kuda ke zebra). Struktur ini memastikan realisme dan pelestarian konten melalui konsistensi siklus.\n",
        "   - **Discriminator PatchGAN**: Discriminator dalam CycleGAN menggunakan arsitektur PatchGAN, yang menilai patch dari gambar daripada keseluruhan gambar, sehingga memberikan realisme yang lebih terperinci dan lokal.\n",
        "   - **Arsitektur Generator**: Generator dalam CycleGAN mengadopsi arsitektur U-Net dan DCGAN, melibatkan downsampling (pengkodean), upsampling (dekoding), dan lapisan konvolusi dengan normalisasi batch dan ReLU. Generator ini dilengkapi dengan lapisan konvolusi tambahan dan koneksi residual untuk mendukung transformasi yang lebih dalam.\n",
        "\n",
        "4. **Loss Fungsi Utama**:\n",
        "   - **Cycle Consistency Loss**: Memastikan bahwa gaya gambar dapat diubah dan kemudian dikembalikan ke bentuk aslinya dengan kehilangan detail atau konten yang minimal. Ini dicapai dengan meminimalkan perbedaan piksel antara gambar asli dan gambar yang dikembalikan.\n",
        "   - **Adversarial Loss**: Digunakan untuk memastikan bahwa gambar yang dihasilkan oleh generator tampak realistis.\n",
        "   - **Identity Loss**: Merupakan istilah loss opsional yang bertujuan untuk meningkatkan pelestarian warna dalam gambar yang dihasilkan. Ini memastikan bahwa gambar input ke generator harus menghasilkan gambar yang sama jika sudah berada dalam gaya target.\n",
        "\n",
        "5. **Implementasi**:\n",
        "   - CycleGAN melibatkan dua tahap transformasi, di mana gambar diubah dari satu gaya ke gaya lain dan kemudian dikembalikan ke gaya aslinya. Model ini berusaha agar gambar akhir yang dikembalikan mirip dengan gambar asli.\n",
        "\n",
        "6. **Aplikasi CycleGAN**:\n",
        "   - CycleGAN dapat digunakan dalam berbagai aplikasi, termasuk transfer gaya, pengeditan gambar, dan penerjemahan domain, seperti mengubah gambar musim dingin menjadi musim panas atau mengubah foto menjadi lukisan."
      ],
      "metadata": {
        "id": "R-OuK49SErn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pengenalan Diffusion Models\n",
        "\n",
        "1. **Definisi Diffusion Models**:\n",
        "   - Diffusion models adalah jenis model generatif yang bekerja dengan cara memodelkan proses difusi data. Mereka menghasilkan data baru dengan memulai dari noise acak dan secara bertahap mengubahnya menjadi data yang lebih terstruktur melalui serangkaian langkah.\n",
        "\n",
        "2. **Proses Difusi**:\n",
        "   - Proses difusi terdiri dari dua fase utama:\n",
        "     - **Fase Forward (Noising)**: Data asli secara bertahap dicampur dengan noise, sehingga pada akhirnya menjadi noise murni. Proses ini melibatkan penambahan noise Gaussian ke data asli dalam beberapa langkah.\n",
        "     - **Fase Reverse (Denoising)**: Model dilatih untuk membalikkan proses forward, yaitu mengubah noise menjadi data yang lebih terstruktur. Ini dilakukan dengan memprediksi dan menghapus noise dari data yang terkontaminasi.\n",
        "\n",
        "3. **Arsitektur Model**:\n",
        "   - Diffusion models sering menggunakan arsitektur jaringan saraf, seperti U-Net, untuk memfasilitasi proses denoising. Jaringan ini dilatih untuk memprediksi noise yang ditambahkan pada setiap langkah, sehingga dapat menghilangkan noise secara efektif.\n",
        "\n",
        "4. **Keunggulan Diffusion Models**:\n",
        "   - Diffusion models telah menunjukkan hasil yang sangat baik dalam menghasilkan gambar berkualitas tinggi, sering kali melampaui kualitas yang dihasilkan oleh GANs dan VAEs dalam beberapa aplikasi.\n",
        "   - Mereka juga lebih stabil dalam pelatihan dibandingkan dengan GANs, yang sering kali menghadapi masalah seperti mode collapse.\n",
        "\n",
        "5. **Aplikasi Diffusion Models**:\n",
        "   - Diffusion models dapat digunakan dalam berbagai aplikasi, termasuk:\n",
        "     - **Generasi Gambar**: Menghasilkan gambar realistis dari noise.\n",
        "     - **Penerjemahan Gaya**: Mengubah gambar dari satu gaya ke gaya lain.\n",
        "     - **Super-Resolusi**: Meningkatkan resolusi gambar dengan menghilangkan noise.\n",
        "\n",
        "6. **Contoh Model Diffusion**:\n",
        "   - Beberapa model diffusion yang terkenal termasuk DALL-E 2 dan Stable Diffusion, yang telah digunakan untuk menghasilkan gambar dari deskripsi teks dan menghasilkan gambar berkualitas tinggi."
      ],
      "metadata": {
        "id": "t5RRNl-HE9M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pengenalan Stable Diffusion\n",
        "\n",
        "1. **Definisi Stable Diffusion**:\n",
        "   - Stable Diffusion adalah model generatif berbasis difusi yang mampu menghasilkan gambar fotorealistik dari input teks. Model ini menggunakan pendekatan latent diffusion, yang memungkinkan efisiensi dalam menghasilkan gambar berkualitas tinggi.\n",
        "\n",
        "2. **Cara Kerja**:\n",
        "   - Model ini memulai dari noise acak dan secara bertahap mengubahnya menjadi gambar yang lebih terstruktur melalui proses denoising. Proses ini melibatkan dua fase: fase forward (menambahkan noise) dan fase reverse (menghilangkan noise).\n",
        "\n",
        "3. **Arsitektur Model**:\n",
        "   - Stable Diffusion menggunakan encoder teks (seperti CLIP) untuk memahami input teks dan menghasilkan representasi yang digunakan dalam proses generasi gambar.\n",
        "\n",
        "4. **Aplikasi**:\n",
        "   - Model ini dapat digunakan untuk berbagai aplikasi, termasuk generasi seni, desain, dan eksplorasi kreatif.\n",
        "\n",
        "### Kode untuk Menggunakan Stable Diffusion\n",
        "\n",
        "Berikut adalah penjelasan tentang kode yang digunakan untuk menjalankan model Stable Diffusion menggunakan library `diffusers` dari Hugging Face:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Mengatur model ID dan perangkat (GPU)\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\"  # atau \"cpu\" jika tidak menggunakan GPU\n",
        "\n",
        "# Memuat pipeline Stable Diffusion\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Menentukan prompt untuk generasi gambar\n",
        "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "\n",
        "# Menghasilkan gambar\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Menyimpan gambar ke file\n",
        "image.save(\"astronaut_rides_horse.png\")\n",
        "```\n",
        "\n",
        "#### Penjelasan Kode:\n",
        "\n",
        "1. **Import Library**:\n",
        "   - `torch`: Digunakan untuk komputasi tensor dan memanfaatkan GPU.\n",
        "   - `StableDiffusionPipeline`: Kelas dari library `diffusers` yang digunakan untuk memuat dan menjalankan model Stable Diffusion.\n",
        "\n",
        "2. **Pengaturan Model dan Perangkat**:\n",
        "   - `model_id`: Menyimpan ID model yang akan digunakan.\n",
        "   - `device`: Menentukan perangkat yang akan digunakan (GPU atau CPU).\n",
        "\n",
        "3. **Memuat Pipeline**:\n",
        "   - `StableDiffusionPipeline.from_pretrained()`: Memuat model dari Hugging Face Model Hub. Parameter `torch_dtype=torch.float16` digunakan untuk mengurangi penggunaan memori GPU.\n",
        "\n",
        "4. **Menentukan Prompt**:\n",
        "   - `prompt`: Teks yang digunakan sebagai input untuk menghasilkan gambar.\n",
        "\n",
        "5. **Menghasilkan Gambar**:\n",
        "   - `pipe(prompt)`: Memanggil pipeline dengan prompt yang diberikan untuk menghasilkan gambar. Hasilnya disimpan dalam variabel `image`.\n",
        "\n",
        "6. **Menyimpan Gambar**:\n",
        "   - `image.save()`: Menyimpan gambar yang dihasilkan ke dalam file dengan nama yang ditentukan.\n",
        "\n",
        "### Catatan Tambahan\n",
        "- Jika Anda memiliki keterbatasan memori GPU (kurang dari 4GB), pastikan untuk memuat pipeline dalam presisi float16 untuk mengurangi penggunaan memori.\n",
        "- Anda juga dapat mengganti scheduler untuk proses denoising dengan menggunakan `EulerDiscreteScheduler` atau scheduler lainnya yang tersedia.\n"
      ],
      "metadata": {
        "id": "i251LErMFKT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pengenalan Diffusion Models\n",
        "\n",
        "1. **Apa itu Diffusion Models?**:\n",
        "   - Diffusion models adalah jenis model generatif yang menghasilkan data baru dengan memulai dari noise acak dan secara bertahap mengubahnya menjadi data yang lebih terstruktur. Proses ini mirip dengan cara fisik di mana partikel menyebar dan kemudian kembali ke keadaan teratur.\n",
        "\n",
        "2. **Proses Kerja**:\n",
        "   - **Fase Forward (Noising)**: Data asli dicampur dengan noise secara bertahap hingga menjadi noise murni. Ini dilakukan dalam beberapa langkah, di mana setiap langkah menambahkan sedikit noise.\n",
        "   - **Fase Reverse (Denoising)**: Model dilatih untuk membalikkan proses ini, yaitu mengubah noise menjadi data yang lebih terstruktur. Model belajar untuk menghapus noise dari data yang terkontaminasi.\n",
        "\n",
        "3. **Mengapa Diffusion Models Efektif?**:\n",
        "   - Diffusion models telah menunjukkan hasil yang sangat baik dalam menghasilkan gambar berkualitas tinggi. Mereka lebih stabil dalam pelatihan dibandingkan dengan model generatif lainnya seperti GANs, yang sering kali menghadapi masalah seperti mode collapse.\n",
        "\n",
        "### Kode untuk Menggunakan Diffusion Models\n",
        "\n",
        "Berikut adalah contoh kode sederhana untuk menggunakan model diffusion, seperti Stable Diffusion, dengan library `diffusers` dari Hugging Face:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Mengatur model ID dan perangkat (GPU)\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "device = \"cuda\"  # atau \"cpu\" jika tidak menggunakan GPU\n",
        "\n",
        "# Memuat pipeline Stable Diffusion\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# Menentukan prompt untuk generasi gambar\n",
        "prompt = \"a fantasy landscape with mountains and rivers\"\n",
        "\n",
        "# Menghasilkan gambar\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Menyimpan gambar ke file\n",
        "image.save(\"fantasy_landscape.png\")\n",
        "```\n",
        "\n",
        "#### Penjelasan Kode:\n",
        "\n",
        "1. **Import Library**:\n",
        "   - `torch`: Digunakan untuk komputasi tensor dan memanfaatkan GPU.\n",
        "   - `StableDiffusionPipeline`: Kelas dari library `diffusers` yang digunakan untuk memuat dan menjalankan model Stable Diffusion.\n",
        "\n",
        "2. **Pengaturan Model dan Perangkat**:\n",
        "   - `model_id`: Menyimpan ID model yang akan digunakan. Dalam hal ini, kita menggunakan model Stable Diffusion.\n",
        "   - `device`: Menentukan perangkat yang akan digunakan (GPU atau CPU).\n",
        "\n",
        "3. **Memuat Pipeline**:\n",
        "   - `StableDiffusionPipeline.from_pretrained()`: Memuat model dari Hugging Face Model Hub. Parameter `torch_dtype=torch.float16` digunakan untuk mengurangi penggunaan memori GPU.\n",
        "\n",
        "4. **Menentukan Prompt**:\n",
        "   - `prompt`: Teks yang digunakan sebagai input untuk menghasilkan gambar. Anda dapat mengganti teks ini dengan deskripsi lain sesuai keinginan.\n",
        "\n",
        "5. **Menghasilkan Gambar**:\n",
        "   - `pipe(prompt)`: Memanggil pipeline dengan prompt yang diberikan untuk menghasilkan gambar. Hasilnya disimpan dalam variabel `image`.\n",
        "\n",
        "6. **Menyimpan Gambar**:\n",
        "   - `image.save()`: Menyimpan gambar yang dihasilkan ke dalam file dengan nama yang ditentukan.\n",
        "\n",
        "### Catatan Tambahan\n",
        "- Pastikan Anda memiliki semua dependensi yang diperlukan, termasuk `torch` dan `diffusers`, yang dapat diinstal menggunakan pip.\n",
        "- Jika Anda tidak memiliki GPU, Anda dapat mengubah `device` menjadi `\"cpu\"`, tetapi proses generasi gambar mungkin akan lebih lambat."
      ],
      "metadata": {
        "id": "T2fq509BFeFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Isu Etika dalam Model Generatif\n",
        "\n",
        "1. **Privasi, Bias, dan Kekhawatiran Sosial**:\n",
        "   - Dengan adopsi luas alat pengeditan gambar berbasis AI, muncul kekhawatiran signifikan terkait privasi, bias, dan dampak sosial. Alat ini dapat memanipulasi gambar 2D dan 3D dengan realisme yang luar biasa, yang menimbulkan dilema etis.\n",
        "\n",
        "2. **Dampak pada Masyarakat**:\n",
        "   - **Mengurangi Kepercayaan pada Media**: Deepfakes dan video yang dimanipulasi dapat menyebarkan informasi yang salah dan mengikis kepercayaan publik terhadap berita dan konten online.\n",
        "   - **Pelecehan dan Pencemaran Nama Baik**: Pelaku jahat dapat menggunakan alat AI untuk membuat gambar palsu yang merugikan individu.\n",
        "   - **Standar Kecantikan yang Tidak Realistis**: Alat AI dapat digunakan untuk mengedit gambar agar sesuai dengan standar kecantikan yang tidak realistis, yang berdampak negatif pada harga diri dan citra tubuh.\n",
        "\n",
        "3. **Pendekatan Saat Ini untuk Mengatasi Masalah**:\n",
        "   - **Transparansi dan Pelabelan**: Pengembang dan platform didorong untuk transparan tentang penggunaan gambar yang diedit AI dan menerapkan sistem pelabelan untuk membedakan konten asli dan yang dimanipulasi.\n",
        "   - **Pemeriksaan Fakta dan Verifikasi**: Media dan perusahaan teknologi berinvestasi dalam alat pemeriksaan fakta untuk membantu memerangi penyebaran informasi yang salah.\n",
        "   - **Kerangka Hukum**: Pemerintah mempertimbangkan langkah-langkah legislatif untuk mengatur penggunaan gambar yang diedit AI dan mempertanggungjawabkan individu atas penyalahgunaannya.\n",
        "\n",
        "4. **Masa Depan**:\n",
        "   - **Teknik Deteksi dan Mitigasi yang Lebih Canggih**: Peneliti diharapkan mengembangkan teknik yang lebih canggih untuk mendeteksi dan mengurangi bahaya yang terkait dengan gambar yang diedit AI.\n",
        "   - **Kesadaran Publik dan Pendidikan**: Kampanye kesadaran publik dan inisiatif pendidikan akan penting untuk mempromosikan penggunaan yang bertanggung jawab terhadap gambar yang diedit AI dan memerangi penyebaran informasi yang salah.\n",
        "   - **Melindungi Hak Seniman Gambar**: Perusahaan yang melatih model besar untuk teks-ke-gambar menghadapi tuntutan hukum karena mengambil karya seniman dari internet tanpa memberikan kredit. Teknik seperti pencemaran gambar (image poisoning) sedang diteliti untuk mengatasi masalah ini.\n",
        "\n",
        "5. **Kesimpulan**:\n",
        "   - Halaman ini menekankan pentingnya memahami isu etika dan bias dalam pengembangan dan penggunaan model generatif. Dengan kemajuan teknologi, penting untuk tetap terinformasi tentang perkembangan terbaru dan dampaknya terhadap masyarakat.\n"
      ],
      "metadata": {
        "id": "4cBJw6qiFsbv"
      }
    }
  ]
}